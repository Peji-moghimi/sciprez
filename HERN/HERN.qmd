---
title: "Antibody-antigen Docking and Design via Hierarchical Equivariant Refinement Networks (HERN)"
subtitle: "Presentation created by <b>Pejvak Moghimi</b> for InstaDeep"
format:
  revealjs:
    incremental: false   
    slide-number: true
    chalkboard: 
      buttons: true
    preview-links: true
    footer: <https://github.com/Peji-moghimi/sciprez>
    css: HERN_files/libs/quarto-diagram/mermaid.css
    pandoc_args: --citeproc
    bibliography: HERN_files/references.bib
    csl: https://www.zotero.org/styles/nature
---

# Background and Motivations {.centered-title}
::: nonincremental
  - PhD in machine learning and immunoinformatics
    - Deep noisy long-tailed learning of antibody convergence across AIRRs
:::

# Introduction {.centered-title}

  - Antibodies, Paratopes, Epitopes and docking
  - Graph Neural Networks (GNNs) and Equivariant GNNs (EGNNs)

## Introduction to Antibodies {.centered-title}

## Antibodies, Paratopes, Epitopes and docking {background-color="black" background-image="HERN_files/images/figure_antibody_structure_cropped.png" background-size="550px" background-repeat="repeat" data-background-opacity="0.3"}
  
  - Antibodies are Y-shaped proteins that bind to antigens.
  - The antibody's binding-surface is called the paratope.
  - An antigen's binding-surfaces are called the epitopes.
    - An antigen can have multiple epitopes.
  - Docking is the process of finding the best binding pose of a paratope to an epitope.
  ![](HERN_files/images/figure_antibody_binding.PNG){.absolute .fragment bottom="10" right="1050" width="225".fade-in-then-out}

## Rigid-body and Semi-flexible Docking {.centered-title}

<table class="custom-table">
  <tr>
    <td>
      ![Rigid-body](HERN_files/images/HADDOCKanimation_of_rigid-body_minimization.gif)
    </td>
    <td>
      ![Semi-flexible simulated annealing](HERN_files/images/haddock_animation_of_semi-flexible_simulated_annealing.gif)
    </td>
  </tr>
</table>

::: footer
From Bonvin lab: [HADDOCK2.4 Antibody - Antigen tutorial](https://www.bonvinlab.org/education/HADDOCK24/HADDOCK24-antibody-antigen/)
:::

## Introduction to GNNs and EGNNs {.centered-title}

## Relevant Types of Graphs

![Point could graph](HERN_files/images/multigraph_exmp.png){.absolute .fragment bottom="300" right="10" width="300".fade-in}
![Point could graph](HERN_files/images/point_could_graph.PNG){.absolute .fragment bottom="30" right="200" width="600".fade-in}
![Hierarchical gaph](HERN_files/images/Hierarchical_graph.PNG){.absolute .fragment bottom="300" right="400" width="600".fade-in}

## Molecules as Graphs {.centered-title}

  - The nodes in the graph represent the atoms of the molecule.
  - The edges represent the bonds between the atoms.
  - The graph structure captures the spatial information of the molecule.

## {background-color="white"}

![](HERN_files/images/caffeine/1680175220705.jpg){.absolute .fragment bottom="50" right="30" width="900".fade-in-then-out}

![](HERN_files/images/caffeine/1680175220679.png){.absolute .fragment bottom="50" right="30" width="900"}

![](HERN_files/images/caffeine/caffeine_cartoon_molecule.png){.absolute .fragment bottom="50" right="30" width="900"}

## Graph Neural Networks (GNNs) {.centered-title}

::: {style="font-size: 0.9em"}
  - GNNs are a class of neural networks, based on the idea of message passing, that operate on graph-structured data.
  - GNNs have been successfully applied to protein design, molecular docking, drug design, and protein-protein interaction, and other applications.

  - GNNs can be used to model complex molecular interactions.
    - The atoms (nodes/vertices) of the molecular graph can be represented by a set of features, and the bonds (edges) can also be represented by a set of features.
    - The features of the atoms and bonds can be updated by message passing.
:::

## Message Passing in GNNs {.centered-title}

::: {style="font-size: 0.9em"}
  - Message passing is the process of propagating information through a graph. 
  - Defines nodes' and edges' features in terms of the neighbouring nodes' and edges' features.
  - The message passing is performed in different ways, multiple times in parallel and a hierarchical manner.
  - Nodes' (Atoms) and edges' (bonds) features are updated through message passing, resulting in an embedding representation that captures the global and/or local spatial information of the molecule. 
  - It can be used in both supervised and unsupervised learning.
:::

## Message Passing in GNNs  {.centered-title}

- Message computation:
  - $m_{u->v}^t = M(h_u^t, h_v^t, e_{u->v})$
- Message aggregation:
  - $a_v^t = A({m_{u->v}^t | u ∈ N(v)})$
- Node update:
  - $h_v^{t+1} = U(h_v^t, a_v^t)$

::: {.notes}
- The message passing process in GNNs typically consists of three steps: message computation, message aggregation, and node update. Let's go through each step:

  - Message computation: In this step, messages are computed for each directed edge (u, v) in the graph, where u is the sender node and v is the receiver node. The messages are usually computed as a function of the features of the sender node and the edge:
    - $m_{u->v}^t = M(h_u^t, h_v^t, e_{u->v})$
    - Here, m_{u->v}^t denotes the message sent from node u to node v at iteration t, h_u^t and h_v^t are the feature vectors of nodes u and v at iteration t, e_{u->v} is the feature vector of the edge from node u to node v, and M is the message function.
  - Message aggregation: In this step, messages received by each node are aggregated to compute an aggregated message for each node:
    - a_v^t = A({m_{u->v}^t | u ∈ N(v)})
    - Here, a_v^t denotes the aggregated message for node v at iteration t, N(v) represents the set of neighboring nodes of v, and A is the aggregation function.
  - Node update: In this step, the aggregated messages are used to update the feature vectors of the nodes:
    - h_v^{t+1} = U(h_v^t, a_v^t)
    - Here, h_v^{t+1} denotes the updated feature vector of node v at iteration t+1, and U is the update function.
:::

## Oversmoothing in Naive/Standard Deep GNNs {.slide-title-60 .centered-title}

<table class="custom-table">
  <tr>
    <td>
      ![](HERN_files/animations/caffeine_graph_Gauss-bluring.gif)
    </td>
    <td>
      ![](HERN_files/animations/standard_GNNs.gif)
    </td>
  </tr>
</table>

## Equivariant Graph Neural Networks (EGNNs) {.centered-title}

::: {style="font-size: 0.85em"}
  - Equivariance is a property of a function that allows it to be invariant to a certain transformation depending on the transformation group, e.g. SO(3) and E(3).
  - EGNNs are GNNs that are equivariant to graph isomorphism.
    - isomorphism is a transformation that preserves the graph structure.
    - I.e. EGNNs are equivariant to group structure and the order of nodes in the graph.
  - Crucially, EGNNs explicitly encode the graph's symmetries and local structure into their architecture by employing specialized layers or operations that are designed to be sensitive to the graph's structure and the specified symmetries.
  - HERN simultaneously folds and docks the paratope.
:::

# Hierarchical Equivariant Iterative Refinement Network (HERN) {.centered-title}

## Overview {.centered-title}

## Problem Formulation {.centered-title}

::: {style="font-size: 0.65em" .incremental}
- Given an epitope structure and a paratope sequence:
  - Find the best docking pose of the paratope to the epitope, formulated as a Joint 3D Point Cloud Completion Task.
- Given an epitope structure:
  - Generate a paratope sequence with high affinity to the epitope.
- Given a training set of antibody-antigen complexes, HERN learns to append an epitope structure $\mathcal{G}_b$ with paratope atoms to form a binding interface $\mathcal{G}_{a, b}$.
:::
::: {.notes}
  - Given a paratope-epitope complex: find the best docking pose of the paratope to the epitope.
  - The problem is formulated as a graph neural network problem.
  - The input consists of a paratope-epitope represented as a graph.
  - The output consists of the best docking pose of the paratope to the epitope.
:::

## Paratope and Epitope {.centered-title}

::: {style="font-size: 0.65em" .incremental}
- A paratope is a sequence of residuesin the complementarity determining regions (CDRs) of an antibody: 
  - $\boldsymbol{a}=\boldsymbol{a}_1 \cdots a_n$ 
- An epitope is composed of $m$ residues that are closest to a paratope:
  - $\boldsymbol{b}=\boldsymbol{b}_1 \boldsymbol{b}_2 \cdots \boldsymbol{b}_m$ 
  - It is a subsequence of an antigen:
    - $\boldsymbol{c}=\boldsymbol{c}_1 \boldsymbol{c}_2 \cdots \boldsymbol{c}_M$, 
    - where $\boldsymbol{b}_i=\boldsymbol{c}_{e_i}$ and $e_i$ is the index of epitope residue $\boldsymbol{b}_i$ in the antigen.
:::
::: {.notes}
  - The paratope is represented by two graphs:
    - A graph to represent the paratope in terms of its residues.
      - Each node in the graph only represents $C_{\alpha}$ atom of the residue.
    - A graph to represent the paratope in terms of its atoms, including the side chains. 
  - Each node in the graph represents an atom.
  - Each edge in the graph represents a bond between two atoms.
  - The nodes in the graph are represented as feature vectors.
  - The features of each node consist of the atomic coordinates.
  - The edges in the graph are represented as feature vectors.
  - The features of each edge consist of the bond type.
:::

## Paratope and Epitope {.centered-title}

::: {style="font-size: 0.85em"}
- The epitope 3D structure $\mathcal{G}_b$ is described as a point cloud of atoms:
  - $\left\{\boldsymbol{b}_{i, j}\right\}_{1 \leq i \leq m, 1 \leq j \leq n_i}$
    - where $n_i$ is the number of atoms in residue $\boldsymbol{b}_i$. 
- The 3D structure of a paratope and a paratope-epitope binding interface is denoted as $\mathcal{G}_a$ and $\mathcal{G}_{a, b}$, respectively. 
  - The first four atoms in any residue correspond to its backbone atoms $\left(\mathrm{N}, \mathrm{C}_\alpha, \mathrm{C}, \mathrm{O}\right)$ and the rest are its side chain atoms. 
  - The 3D coordinate of an atom $\boldsymbol{b}_{i, k}$ is denoted as $\boldsymbol{x}\left(\boldsymbol{b}_{i, k}\right) \in \mathbb{R}^3$.
:::
::: {.notes}
  - The paratope is represented by two graphs:
    - A graph to represent the paratope in terms of its residues.
      - Each node in the graph only represents $C_{\alpha}$ atom of the residue.
    - A graph to represent the paratope in terms of its atoms, including the side chains. 
  - Each node in the graph represents an atom.
  - Each edge in the graph represents a bond between two atoms.
  - The nodes in the graph are represented as feature vectors.
  - The features of each node consist of the atomic coordinates.
  - The edges in the graph are represented as feature vectors.
  - The features of each edge consist of the bond type.
:::

## Docking

## Step 0 setting Node Coordinates {.centered-title}

::: {style="font-size: 0.55em"}
- Random initialization:
$$
\boldsymbol{x}^{(0)}\left(\boldsymbol{a}_{i, k}\right)=\frac{1}{m} \sum_i \boldsymbol{x}\left(\boldsymbol{b}_{i, 1}\right)+\epsilon, \quad \epsilon \sim \mathcal{N}(0,1),
$$
- Distance-based initialization:
$$
\boldsymbol{D}_{i, j}= \begin{cases}\left\|\boldsymbol{h}^{(0)}\left(\boldsymbol{a}_i\right)-\boldsymbol{h}^{(0)}\left(\boldsymbol{a}_j\right)\right\| & i, j \leq n \\ \left\|\boldsymbol{h}^{(0)}\left(\boldsymbol{a}_i\right)-\boldsymbol{h}\left(\boldsymbol{b}_j\right)\right\| & i \leq n, j>n \\ \left\|\boldsymbol{x}\left(\boldsymbol{b}_i\right)-\boldsymbol{x}\left(\boldsymbol{b}_j\right)\right\| & i, j>n\end{cases}
$$

- Eigenvalue decomposition:
$$
\tilde{\boldsymbol{D}}_{i, j}=0.5\left(\boldsymbol{D}_{i, 1}^2+\boldsymbol{D}_{1, j}^2-\boldsymbol{D}_{i, j}^2\right), \quad \tilde{\boldsymbol{D}}=\boldsymbol{U} \boldsymbol{S} \boldsymbol{U}^{\top}
$$
:::
::: {.notes}
- Random initialization: Randomly initialize all coordinates by adding a small Gaussian noise around the center of the epitope $\left(b_{i, 1}\right.$ means the $\mathrm{C}_\alpha$ atom of residue $\left.\boldsymbol{b}_i\right)$
$$
\boldsymbol{x}^{(0)}\left(\boldsymbol{a}_{i, k}\right)=\frac{1}{m} \sum_i \boldsymbol{x}\left(\boldsymbol{b}_{i, 1}\right)+\epsilon, \quad \epsilon \sim \mathcal{N}(0,1),
$$
where the epitope center is defined as the mean of epitope $\mathrm{C}_\alpha$ atoms. From now on, we will abbreviate paratope coordinates $\boldsymbol{x}^{(0)}\left(\boldsymbol{a}_{i, k}\right)$ as $\boldsymbol{x}_{i, k}^{(0)}$ to simplify notation.

- Distance-based initialization: Another strategy is to directly predict the pairwise distance $\boldsymbol{D} \in \mathbb{R}^{(n+m) \times(n+m)}$ between paratope and epitope atoms and reconstruct atom coordinates from $\boldsymbol{D}$. Specifically, let $\boldsymbol{h}^{(0)}\left(\boldsymbol{a}_i\right)=\operatorname{FFN}\left(\boldsymbol{f}\left(\boldsymbol{a}_i\right)\right)$ be the initial representation of a paratope residue. We predict the pairwise distance as the following:
$$
\boldsymbol{D}_{i, j}= \begin{cases}\left\|\boldsymbol{h}^{(0)}\left(\boldsymbol{a}_i\right)-\boldsymbol{h}^{(0)}\left(\boldsymbol{a}_j\right)\right\| & i, j \leq n \\ \left\|\boldsymbol{h}^{(0)}\left(\boldsymbol{a}_i\right)-\boldsymbol{h}\left(\boldsymbol{b}_j\right)\right\| & i \leq n, j>n \\ \left\|\boldsymbol{x}\left(\boldsymbol{b}_i\right)-\boldsymbol{x}\left(\boldsymbol{b}_j\right)\right\| & i, j>n\end{cases}
$$
Intuitively, if $i$ belongs to the paratope ( $i \leq n$ ) and $j$ to the epitope $(j>n), \boldsymbol{D}_{i, j}$ is the Euclidean distance between $\boldsymbol{h}_{\text {seq }}\left(\boldsymbol{a}_i\right)$ and the antigen encoding $\boldsymbol{h}\left(\boldsymbol{c}_{e_j}\right)$. Similarly, the distance between two paratope residues is modeled as the Euclidean distance between their sequence representations $h_{\text {seq }}$. The distance between two epitope residues are directly calculated from their given coordinates $\boldsymbol{x}\left(\boldsymbol{b}_i\right)$.

- Eigenvalue decomposition: Given this pairwise distance matrix $\boldsymbol{D}$, we can obtain the 3D coordinates of each residue via eigenvalue decomposition of the following Gram matrix (Crippen \& Havel, 1978)
$$
\tilde{\boldsymbol{D}}_{i, j}=0.5\left(\boldsymbol{D}_{i, 1}^2+\boldsymbol{D}_{1, j}^2-\boldsymbol{D}_{i, j}^2\right), \quad \tilde{\boldsymbol{D}}=\boldsymbol{U} \boldsymbol{S} \boldsymbol{U}^{\top}
$$
where $S$ is a diagonal matrix with eigenvalues in descending order. The coordinate of each residue $\boldsymbol{a}_i$ is calculated as
$$
\tilde{\boldsymbol{x}}_i^{(0)}=\left[\boldsymbol{X}_{i, 1}, \boldsymbol{X}_{i, 2}, \boldsymbol{X}_{i, 3}\right], \quad \boldsymbol{X}=\boldsymbol{U} \sqrt{\boldsymbol{S}} .
$$
:::

## Step 0 - setting Node Coordinates {.centered-title}

![](HERN_files/images/HERN_step_0.PNG){.absolute .fragment bottom="100" right="300" width="600".fade-in-then-out}
![](HERN_files/images/HERN_step_0_0.5.PNG){.absolute .fragment bottom="100" right="300" width="600".fade-in}

## Step 1 - Hierarchical Encoding {.centered-title}

![](HERN_files/images/HERN_step_1_0.PNG){.absolute .fragment bottom="100" right="300" width="275".fade-in-then-out}
![](HERN_files/images/HERN_step_1_1.PNG){.absolute .fragment bottom="100" right="300" width="600".fade-in}
![](HERN_files/images/HERN_step_1_2.PNG){.absolute .fragment bottom="100" right="300" width="600".fade-in-then-out}
![](HERN_files/images/HERN_step_1_3.PNG){.absolute .fragment bottom="100" right="300" width="600".fade-in}

## Atom-Level Interface Encoder {.centered-title}

- The atom-level encoder keeps all the atoms in the point cloud $\mathcal{G}_{a, b}$ and constructs a $K$ nearest neighbor graph $\mathcal{G}_{a, b}^A$. Each node feature is a one-hot encoding of its atom name (e.g., N, $\mathrm{C}_\alpha, \mathrm{C}_\beta, \mathrm{O}$ ).
- Each edge feature between two atoms $\left(\boldsymbol{a}_{i, k}, \boldsymbol{b}_{j, l}\right)$ is represented as:
  - $$
  \boldsymbol{f}\left(\boldsymbol{a}_{i, k}, \boldsymbol{b}_{j, l}\right)=\operatorname{RBF}\left(\left\|\boldsymbol{x}\left(\boldsymbol{a}_{i, k}\right)-\boldsymbol{x}\left(\boldsymbol{b}_{j, l}\right)\right\|\right)
  $$
- Finally, $\mathcal{G}_{a, b}^A$ is encoded by a MPN to learn a vector representation $\boldsymbol{h}\left(\boldsymbol{a}_{i, k}\right), \boldsymbol{h}\left(\boldsymbol{b}_{j, l}\right)$ for each paratope atom $\boldsymbol{a}_{i, k}$ and epitope atom $\boldsymbol{b}_{j, l}$.

## Residue-Level Interface Encoder {.centered-title}

::: {style="font-size: 0.55em"}
- The residue-level encoder only keeps the $\mathrm{C}_\alpha$ atoms and constructs a $K$ nearest neighbor graph $\mathcal{G}_{a, b}^R$ at the residue level. 
- Each residue is represented by a feature vector $\boldsymbol{f}\left(\boldsymbol{a}_i\right)$ including:
  - Amino acid features. Each amino acid is represented by six features: 
    - polarity $f_p \in\{0,1\}$ 
    - hydropathy $f_h \in[-4.5,4.5]$ 
    - volume $f_v \in[60.1,227.8]$
    - charge $f_c \in\{-1,0,1\}$
    - whether it is a hydrogen bond donor $f_d \in\{0,1\}$ or acceptor $f_a \in\{0,1\}$.
  - For hydropathy and volume features, the radial basis is expanded with interval size 0.1 and 10, respectively
  - &#8594; amino acid feature with 112 dimensions.
- It then concatenates the amino acid features with the sum of atom-level $\boldsymbol {h}\left(\boldsymbol{a}_{i, k}\right)$ within that residue as the initial residue representation:
$$
\begin{aligned}
\tilde{\boldsymbol{f}}\left(\boldsymbol{a}_i\right) & =\boldsymbol{f}\left(\boldsymbol{a}_i\right) \oplus \sum_k \boldsymbol{h}\left(\boldsymbol{a}_{i, k}\right) \\
\tilde{\boldsymbol{f}}\left(\boldsymbol{b}_j\right) & =\boldsymbol{f}\left(\boldsymbol{b}_j\right) \oplus \sum_l \boldsymbol{h}\left(\boldsymbol{b}_{j, l}\right)
\end{aligned}
$$
:::

## Residue-level edge features {.centered-title}

::: {style="font-size: 0.95em"}
  - Next, it builds a local coordinate frame for each residue and compute edge features $\boldsymbol{f}\left(\boldsymbol{a}_i, \boldsymbol{a}_j\right)$ describing the distance, direction, and orientation between nearby residues (Ingraham et al., 2019).
  - The local coordinate frame for each residue $\boldsymbol{a}_i$ is defined as $\boldsymbol{O}_i=\left[\boldsymbol{c}_i, \boldsymbol{n}_i, \boldsymbol{c}_i \times \boldsymbol{n}_i\right]$, where:
    - $\boldsymbol{u}_i=\frac{\boldsymbol{x}_i-\boldsymbol{x}_{i-1}}{\left\|\boldsymbol{x}_i-\boldsymbol{x}_{i-1}\right\|}$
    - $\quad \boldsymbol{c}_i=\frac{\boldsymbol{u}_i-\boldsymbol{u}_{i+1}}{\left\|\boldsymbol{u}_i-\boldsymbol{u}_{i+1}\right\|}$
    - $\quad \boldsymbol{n}_i=\frac{\boldsymbol{u}_i \times \boldsymbol{u}_{i+1}}{\left\|\boldsymbol{u}_i \times \boldsymbol{u}_{i+1}\right\|}$
:::

## Relative Spatial Encodings {.centered-title}

::: {style="font-size: 0.5em"}
  - Ingraham *et al.* [@Ingraham2019-bx] developed invariant and locally informative features 
  - by augmenting $\boldsymbol{x}_i$ with 'orientations' $\boldsymbol{O}_i$; defined in terms of the backbone geometry as:
$$
\boldsymbol{O}_i=\left[\begin{array}{lll}
\boldsymbol{b}_i & \boldsymbol{n}_i & \boldsymbol{b}_i \times \boldsymbol{n}_i
\end{array}\right],
$$
  - where $\boldsymbol{b}_i$ is the negative bisector of angle between the rays $\left(\boldsymbol{x}_{i-1}-\boldsymbol{x}_i\right)$ and $\left(\boldsymbol{x}_{i+1}-\boldsymbol{x}_i\right)$, and $\boldsymbol{n}_i$ is a unit vector normal to that plane. 
  - Formally:
$$
\boldsymbol{u}_i=\frac{\boldsymbol{x}_i-\boldsymbol{x}_{i-1}}{\left\|\boldsymbol{x}_i-\boldsymbol{x}_{i-1}\right\|}, \quad \boldsymbol{b}_i=\frac{\boldsymbol{u}_i-\boldsymbol{u}_{i+1}}{\left\|\boldsymbol{u}_i-\boldsymbol{u}_{i+1}\right\|}, \quad \boldsymbol{n}_i=\frac{\boldsymbol{u}_i \times \boldsymbol{u}_{i+1}}{\left\|\boldsymbol{u}_i \times \boldsymbol{u}_{i+1}\right\|}
$$
  - Finally, they derived the spatial edge features $e_{i j}^{(s)}$ from the rigid body transformation that relates reference frame $\left(\boldsymbol{x}_i, \boldsymbol{O}_i\right)$ to reference frame $\left(\boldsymbol{x}_j, \boldsymbol{O}_j\right)$. 
  - Then, decomposed it into features for distance, direction, and orientation as
$$
\boldsymbol{e}_{i j}^{(s)}=\left(\mathbf{RBF}\left(\left\|\boldsymbol{x}_j-\boldsymbol{x}_i\right\|\right), \quad \boldsymbol{O}_i^T \frac{\boldsymbol{x}_j-\boldsymbol{x}_i}{\left\|\boldsymbol{x}_j-\boldsymbol{x}_i\right\|}, \quad \mathbf{q}\left(\boldsymbol{O}_i^T \boldsymbol{O}_j\right)\right)
$$
  - Here the first vector is a distance encoding $\mathbf{RBF}(\cdot)$ lifted into a radial basis, the second vector is a direction encoding that corresponds to the relative direction of $\boldsymbol{x}_j$ in the reference frame of $\left(\boldsymbol{x}_i, \boldsymbol{O}_i\right)$, and the third vector is an orientation encoding $\mathbf{q}(\cdot)$ of the quaternion representation of the spatial rotation matrix $\boldsymbol{O}_i^T \boldsymbol{O}_j$.
:::
::: {.notes}
  - by first augmenting the points $\boldsymbol{x}_i$ with 'orientations' $\boldsymbol{O}_i$ that define a local coordinate system at each point; defined in terms of the backbone geometry as
  - 
  - $\boldsymbol{u}_i=\frac{\boldsymbol{x}_i-\boldsymbol{x}_{i-1}}{\left\|\boldsymbol{x}_i-\boldsymbol{x}_{i-1}\right\|}$ is the direction.
:::
<div class="slide-footer">[1] Ingraham *et al.*, Adv. Neural Inf. Process. Syst., 2019, <a href="https://papers.nips.cc/paper_files/paper/2019/hash/f3a4ff4839c56a5f460c88cce3666a2b-Abstract.html" target="_blank">DOI</a></div>

## Quaternions {.centered-title .scrollable}

::: {style="font-size: 0.75em"}
  - Quaternions represent 3D rotations as four-element vectors that can be efficiently and reasonably compared by inner products.
  - utilises complex numbers for geometric transformations/rotations in 3D space 
  - Better than Euler angles
    - Does not suffer from Gimble Lock
      - Gimble Lock is the case when the Euler angles are not unique and the rotation is not well defined
:::

## Quaternions {.centered-title}

![](HERN_files/animations/Quaternions.mp4){.absolute .fragment bottom="50" right="30" width="900".fade-out}
![](HERN_files/images/quaternions.png){.absolute .fragment bottom="30" right="30" width="900"}

::: footer
[<b>3blue1brown</b>](https://eater.net/quaternions)
:::

## edge feature computation {.centered-title}

::: {style="font-size: 0.65em"}
  - Based on the local frame, we compute the following edge features
    - $$
    \boldsymbol{f}\left(\boldsymbol{a}_i, \boldsymbol{a}_j\right)=\left(E_{\mathrm{pos}}(i-j), \quad \operatorname{RBF}\left(\left\|\boldsymbol{x}_{i, 1}-\boldsymbol{x}_{j, 1}\right\|\right), \quad \boldsymbol{O}_i^{\top} \frac{\boldsymbol{x}_{j, 1}-\boldsymbol{x}_{i, 1}}{\left\|\boldsymbol{x}_{i, 1}-\boldsymbol{x}_{j, 1}\right\|}, \quad \boldsymbol{q}\left(\boldsymbol{O}_i^{\top} \boldsymbol{O}_j\right)\right)
    $$
:::
::: {.notes}
  - <u><b>Relative positional encodings</b></u>:
    - As in the original Transformer, we also represent distances between residues in the sequence (rather than space) with positional embeddings $\boldsymbol{e}_{i j}^{(p)}$. 
    - Positioning of each neighbor $j$ is relative to the node under consideration $i$
    - As result, we get a position embedding as a sinusoidal function of the gap $i-j$. 
:::

## Message Passing {.centered-title}

::: {style="font-size: 0.65em"}
  - Lastly, the encoder takes the node and edge features into another MPN to compute the final representations $\left\{\boldsymbol{h}\left(\boldsymbol{a}_i\right)\right\},\left\{\boldsymbol{h}\left(\boldsymbol{b}_j\right)\right\}$ for paratope and epitope residues.
  - MPN contains $L$ message passing layers.
  - Let $\mathcal{N}_i$ be the set of neighbor nodes for residue $\boldsymbol{a}_i$
  - MPN layer consists of a standard message passing step followed by an aggregation step with residual connection.
  - $\mathcal{G}_{a, b}^A$ is encoded by a MPN which learns a vector representation $\boldsymbol{h}\left(\boldsymbol{a}_{i, k}\right), \boldsymbol{h}\left(\boldsymbol{b}_{j, l}\right)$ for each paratope atom $\boldsymbol{a}_{i, k}$ and epitope atom $\boldsymbol{b}_{j, l}$.
  - $$
  \boldsymbol{h}_{l+1}\left(\boldsymbol{a}_i\right)=\boldsymbol{h}_l\left(\boldsymbol{a}_i\right)+\sum_{j \in \mathcal{N}_i} \operatorname{FFN}\left(\boldsymbol{h}_l\left(\boldsymbol{a}_i\right), \boldsymbol{h}_l\left(\boldsymbol{a}_j\right), \boldsymbol{f}\left(\boldsymbol{a}_j\right), \boldsymbol{f}\left(\boldsymbol{a}_i, \boldsymbol{a}_j\right)\right) \quad(0 \leq l<L)
  $$
  - $\boldsymbol{h}_l\left(\boldsymbol{a}_i\right)$ is the vector representation of the $i$-th atom $\boldsymbol{a}_i$ after $l$ message passing steps.
:::

## Summary of Hierarchical Encoding {.centered-title}

  - hierarchical encoder outputs a set of residue representations $\left\{\boldsymbol{h}\left(\boldsymbol{a}_i\right), \boldsymbol{h}\left(\boldsymbol{b}_j\right)\right\}$ and atom representations $\left\{\boldsymbol{h}\left(\boldsymbol{a}_{i, k}\right), \boldsymbol{h}\left(\boldsymbol{b}_{j, l}\right)\right\}$. 
  - Crucially, the whole encoding process is rotation and translation invariant.

## Step 2 - Iterative Refinement {.centered-title}

::: {style="font-size: 0.65em" .incremental}
- Directly predicting the 3D coordinates by $x_i=U h_i \in \mathbb{R}^3$ is not equivariant to epitope rotation and translation.
  ![](HERN_files/images/HERN_step_2_0.PNG){.absolute .fragment bottom="10" right="400" width="300".fade-in}
- Instead, we need an equivariant update
- $x_i \leftarrow x_i+\sum_j \underbrace{\phi_x\left(h_i, h_j\right)\left(x_i-x_j\right)}_{\text {Force }}$
  ![](HERN_files/images/HERN_step_2_1.PNG){.absolute .fragment bottom="10" right="400" width="300".fade-in}
- If we rotate the epitope, the force vector will rotate correspondingly
:::

# HERN Architecture{.centered-title}

<style>
/* Center slice title */
.centered-title {
  text-align: center;
}
</style>

##

<style>
.center-diagram {
  width: 125%;
  height: 95%;
  display: flex;
  align-items: center;
  justify-content: center;
}
.slide {
  display: flex;
  align-items: center;
  justify-content: center;
}
.node text {
  font-size: 120px;
}
.label text {
  font-size: 30px;
}
</style>

<section class="slide">

<div class="center-diagram">
```{mermaid}
classDiagram
  class EGNNEncoder {
    - update_X: bool
    - features_type: str
    - features: ProteinFeatures
    - node_in: int
    - edge_in: int
    - W_v: nn.Linear
    - W_e: nn.Linear
    - layers: nn.ModuleList
    - W_x: nn.Linear
    - U_x: nn.Linear
    - T_x: nn.Sequential
    + __init__(args, node_hdim=0, features_type='backbone', update_X=True)
    - forward(X, V, S, A)

  }
  class HierEGNNEncoder {
    - update_X: bool
    - backbone_CA_only: bool
    - clash_step: int
    - residue_mpn: EGNNEncoder
    - atom_mpn: EGNNEncoder
    - W_x: nn.Linear
    - U_x: nn.Linear
    - T_x: nn.Sequential
    - W_a: nn.Linear
    - U_a: nn.Linear
    - T_a: nn.Sequential
    - embedding: nn.Embedding
    + __init__(args, update_X=True, backbone_CA_only=True)
    + forward(X, V, S, A)
  }
  class MPNNLayer {
    - num_hidden: int
    - num_in: int
    - dropout: nn.Dropout
    - norm: nn.Identity
    - W: nn.Sequential
    + forward(h_V: torch.Tensor, h_E: torch.Tensor, mask_attend: torch.Tensor) -> torch.Tensor
  }
  class PosEmbedding {
    - num_embeddings: int
    + forward(E_idx: torch.Tensor) -> torch.Tensor
  }
  class AAEmbedding {
    - hydropathy: dict
    - volume: dict
    - charge: dict
    - polarity: dict
    - acceptor: dict
    - donor: dict
    - embedding: torch.Tensor
    + to_rbf(D: torch.Tensor, D_min: float, D_max: float, stride: float) -> torch.Tensor
    + transform(aa_vecs: torch.Tensor) -> torch.Tensor
    + dim() -> int
    + forward(x: torch.Tensor, raw: bool=False) -> torch.Tensor
    + soft_forward(x: torch.Tensor) -> torch.Tensor
  }
  class ABModel {
    - k_neighbors: int
    - hidden_size: int
    - embedding: AAEmbedding
    - features: ProteinFeatures
    - W_i: nn.Linear
    - bce_loss: nn.BCEWithLogitsLoss
    - ce_loss: nn.CrossEntropyLoss
    - mse_loss: nn.MSELoss
    - huber_loss: nn.SmoothL1Loss
    + select_target(tgt_X: torch.Tensor, tgt_h: torch.Tensor, tgt_A: torch.Tensor, tgt_pos: list) -> tuple
}
  class PositionalEncodings {
    -num_embeddings: int
    -period_range: list
    +__init__(num_embeddings, period_range=[2,1000])
  }
  class ProteinFeatures {
    - top_k: int
    - num_rbf: int
    - features_type: str
    - direction: str
    + forward(positions: torch.Tensor, h_V: torch.Tensor, h_E: torch.Tensor, A: torch.Tensor, R: torch.Tensor, pos: list) -> tuple
  }
  class RefineDocker {
    + rstep: int
    + target_mpn: EGNNEncoder
    + hierarchical: bool
    - U_i: nn.Linear
    - struct_mpn: Union[EGNNEncoder, HierEGNNEncoder]
    - W_x0: nn.Sequential
    - U_x0: nn.Sequential
    + struct_loss(bind_X, tgt_X, true_V, true_R, true_D, inter_D, true_C): Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]
    + forward(binder, target, surface): ReturnType
  }
  class CondRefineDecoder {
    -hierarchical: bool
    -residue_atom14: Tensor
    -W_s0: Sequential
    -W_x0: Sequential
    -U_x0: Sequential
    -W_s: Linear
    -U_i: Linear
    -target_mpn: EGNNEncoder
    -struct_mpn: EGNNEncoder or HierEGNNEncoder
    -seq_mpn: EGNNEncoder or HierEGNNEncoder
    +struct_loss(): Function
    +forward(): ReturnType
    +generate(): ReturnType
  }
  class AttRefineDecoder {
    - W_x: nn.Linear
    - W_s: nn.Linear
    - struct_mpn: EGNNEncoder
    - seq_mpn: EGNNEncoder
    - W_x0: nn.Sequential
    - U_i: nn.Linear
    - target_mpn: EGNNEncoder
    - W_att: nn.Sequential
    + attention(Q: Tensor, context: Tensor, cmask: Tensor): Tensor
    + struct_loss(X: Tensor, mask: Tensor, true_D: Tensor, true_V: Tensor, true_R: Tensor, true_C: Tensor): Tuple
    + forward(binder: Tuple, target: Tuple, surface: Tuple): ReturnType
    + generate(target: Tuple, surface: Tuple): ReturnType
  }
  class UncondRefineDecoder {
    - W_x: nn.Linear
    - W_s: nn.Linear
    - struct_mpn: EGNNEncoder
    - seq_mpn: EGNNEncoder
    - W_x0: nn.Sequential
    + struct_loss(X: Tensor, mask: Tensor, true_D: Tensor, true_V: Tensor, true_R: Tensor, true_C: Tensor): Tuple
    + forward(binder: Tuple, target: Tuple, surface: Tuple): ReturnType
    + generate(target: Tuple, surface: Tuple): ReturnType
  }
  class SequenceDecoder {
    - no_target
    - W_s
    - seq_rnn
    - U_i
    - target_mpn
    - W_att
    + attention(Q, context, cmask)
    + forward(binder, target, surface)
    + generate(target, surface)
  }
  
  HierEGNNEncoder *-- EGNNEncoder : init
  ABModel *-- AAEmbedding : W_i = nn.Linear(self.embedding.dim(), args.hidden_size)
  ABModel *-- ProteinFeatures : init features
  RefineDocker *-- EGNNEncoder : init
  CondRefineDecoder *-- EGNNEncoder : init
  AttRefineDecoder *-- EGNNEncoder : init
  ABModel --|> RefineDocker : inherits dihedral features
  ABModel --|> CondRefineDecoder : Inherits
  ABModel --|> AttRefineDecoder : Inherits
  RefineDocker *-- HierEGNNEncoder : init
  CondRefineDecoder *-- HierEGNNEncoder : init
  ABModel --|> UncondRefineDecoder : Inherits
  ABModel --|> SequenceDecoder : Inherits
  PositionalEncodings --* ProteinFeatures : init
  PosEmbedding --* AttRefineDecoder : init
  PosEmbedding --* CondRefineDecoder : init
  PosEmbedding --* UncondRefineDecoder : init
  EGNNEncoder <|-- MPNNLayer : init
  EGNNEncoder <|-- ProteinFeatures : init
```
</div>
</section>

# Results and Discussion

## 
![](HERN_files/images/hern_table1.PNG)

## 
![](HERN_files/images/hern_table2.PNG)

## 
![](HERN_files/images/HERN_RESULTS.PNG.PNG)

## Issue

- Only models CDRH3
  - Dvelopability!?
- Uses IgFold+HDOCK for comparison
- Does not model end-to-end

## 

![](HERN_files/images/Fv_models_RMSD-comparisons.PNG)

## 

![](HERN_files/images/CDRH3_models_RMSD-comparisons_cropped.png)

## 

![](HERN_files/images/Fv_models_RMSD-comparisons.PNG)

# Refernces
---
nocite: '@*'
---