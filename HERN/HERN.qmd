---
title: "Antibody-antigen Docking and Design via Hierarchical Equivariant Refinement Networks (HERN)"
subtitle: "Presentation created by <b>Pejvak Moghimi</b> for InstaDeep"
format:
  revealjs:
    incremental: true   
    slide-number: true
    chalkboard: 
      buttons: true
    preview-links: true
    footer: <https://github.com/Peji-moghimi/sciprez>
    css: HERN_files/libs/quarto-diagram/mermaid.css
---

# Background and Motivations
::: nonincremental
  - PhD in machine learning and immunoinformatics
    - Deep noisy long-tailed learning of antibody convergence across AIRRs
:::

# Introduction

  - Antibodies, Paratopes, Epitopes and docking
  - Graph Neural Networks (GNNs)
  - Message Passing in GNNs


## Antibodies, Paratopes, Epitopes and docking {background-color="black" background-image="HERN_files/images/figure_antibody_structure_cropped.png" background-size="550px" background-repeat="repeat" data-background-opacity="0.3"}
  
  - Antibodies are Y-shaped proteins that bind to antigens.
  - The antibody's binding-surface is called the paratope.
  - An antigen's binding-surfaces are called the epitopes.
    - An antigen can have multiple epitopes.
  - Docking is the process of finding the best binding pose of a paratope to an epitope.
  ![](HERN_files/images/figure_antibody_binding.PNG){.absolute .fragment bottom="10" right="1050" width="225".fade-in-then-out}

## Molecules as Graphs

  - The nodes in the graph represent the atoms of the molecule.
  - The edges represent the bonds between the atoms.
  - The graph structure captures the spatial information of the molecule.

## {background-color="white"}

![](HERN_files/images/caffeine/1680175220705.jpg){.absolute .fragment bottom="50" right="30" width="900".fade-in-then-out}

![](HERN_files/images/caffeine/1680175220679.png){.absolute .fragment bottom="50" right="30" width="900"}

![](HERN_files/images/caffeine/caffeine_cartoon_molecule.png){.absolute .fragment bottom="50" right="30" width="900"}

## Graph Neural Networks (GNNs)

  - GNNs are a class of neural networks, based on the idea of message passing, that operate on graph-structured data.
  - GNNs have been successfully applied to protein design, molecular docking, drug design, and protein-protein interaction, and other applications.

  - GNNs can be used to model complex molecular interactions.
    - The atoms (nodes/vertices) of the molecular graph can be represented by a set of features, and the bonds (edges) can also be represented by a set of features.
    - The features of the atoms and bonds can be updated by message passing.

## Equivariant Graph Neural Networks (EGNNs)

  - Equivariance is a property of a function that allows it to be invariant to a certain transformation.
  - In the context of GNNs, equivariance is a property of the GNN that allows it to be invariant to a certain transformation on the graph.
  - EGNNs are GNNs that are equivariant to graph isomorphism.
    - isomorphism is a transformation that preserves the graph structure.
    - I.e. EGNNs are invariant to group structure and the order of nodes in the graph.

## Message Passing in GNNs

  - Message passing is the process of propagating information through a graph. 
  - Defines nodes' and/or edges' features in terms of the neighbouring nodes' and/or edges' features.
  - The message passing is performed in different ways, multiple times in parallel and a hierarchical manner.
  - Nodes' (Atoms) and edges' (bonds) features are updated through message passing, resulting in an embedding representation that captures the global and/or local spatial information of the molecule. 
  - It can be used in both supervised and unsupervised learning.

## Message Passing in GNNs {.scrollable}
  - The message passing process in GNNs is based on the following equation:
  $$ 
  \mathbf{h}_i^{\left(t+1\right)} = \sigma\left(\mathbf{W}_h\mathbf{h}_i^{\left(t\right)} + \sum_{j\in \mathcal{N}(i)} \mathbf{W}_e \mathbf{e}_{ij}^{\left(t\right)}\right)
  $$
  - Where $\mathbf{h}_i^{\left(t\right)}$ is the node feature vector at time $t$ for node $i$, where time is the number of message passing steps.
  - $\mathbf{W}_h$ is the weight matrix for the node features.
  - $\mathbf{e}_{ij}^{\left(t\right)}$ is the edge feature vector at time $t$ between node $i$ and node $j$.
  - $\mathbf{W}_e$ is the weight matrix for the edge features.
  - $\mathcal{N}(i)$ is the set of neighbors of node $i$.
  - $\sigma$ is the activation function.

## Message Passing in GNNs
  - Message passing steps are as follows:
    - Reading the node features $\mathbf{h}_i^{\left(t\right)}$.
    - Reading the edge features $\mathbf{e}_{ij}^{\left(t\right)}$.
    - Aggregating the node features by an aggregation function $\mathcal{A}$.
    - Updating the node features by a message function $\mathcal{M}$.
    - Passing the updated node features to the next node.
    - For each node $i$ in the graph, update its feature vector $h_i$ with the features of its neighbours.

## Problem Formulation

  - Given a protein-protein complex and a paratope, find the best docking pose of the paratope to the epitope.
  - The problem is formulated as a graph neural network problem.
  - The input consists of a paratope-epitope represented as a graph.
  - The output consists of the best docking pose of the paratope to the epitope.

## Paratope-Epitope Complex

  - The paratope is represented by two graphs:
    - A graph to represent the paratope in terms of its residues.
      - Each node in the graph only represents $C_{/alpha}$ atom of the residue.
    - A graph to represent the paratope in terms of its atoms, including the side chains. 
  - Each node in the graph represents an atom.
  - Each edge in the graph represents a bond between two atoms.
  - The nodes in the graph are represented as feature vectors.
  - The features of each node consist of the atomic coordinates.
  - The edges in the graph are represented as feature vectors.
  - The features of each edge consist of the bond type.

## Paratope

  - A paratope consists of a set of amino acids.
  - The amino acids are represented as nodes in the graph.
  - Each node in the graph could represent a residue or an atom.
  - Each edge in the graph represents a bond between two residues or atoms.

# Methods

  - Equivariant Graph Neural Networks (EGNNs)
  - Hierarchical Equivariant Graph Neural Networks (HEGNNs)
  - Refinement Networks
  - Hierarchical Equivariant Refinement Networks (HERN)

## Hierarchical Equivariant Refinement Network (HERN)
  
  - Refinement Networks are neural networks that iteratively refine a set of initial coordinates.
  - The refinement process consists of:
    - Predicting the next residue in the sequence.
    - Predicting the next set of coordinates based on the current coordinates and the predicted residue.
    - Computing a loss function based on the predicted and the current coordinates.
  - The refinement process is repeated until the sequence is fully predicted.
  - The message passing is performed in a hierarchical manner for computational efficieny.

# Results

  - Antibody-antigen docking
  - Antibody-antigen design

=======


# Methods

  - Equivariant Graph Neural Networks
  - Hierarchical Equivariant Refinement Networks
  - Antibody-Antigen Docking and Design



=======


# Methods

  - Equivariant Graph Neural Networks (EGNNs)
  - Equivariant Refinement Networks (ERNs)
  - Hierarchical Equivariant Refinement Networks (HERNs)
  - Training and evaluation


## Training and evaluation

  - The model is trained on a large dataset of antibody-antigen complexes.
  - The model is evaluated on a small dataset of antibody-antigen complexes.
  - The model is evaluated on a small dataset of antibody-antigen complexes.
  - The model is evaluated on a small dataset of antibody-antigen complexes.

# Results

  - HERN is able to dock antibodies to antigens.

=======

# Methods

  - Hierarchical Equivariant Refinement Networks (HERN)
  - Equivariant Graph Neural Networks (EGNNs)
  - Unconditional Refinement
  - Sequence Prediction
  - Data Processing
  - Model Training
  - Prediction
  - Relaxation


## Unconditional Refinement

  - Unconditional refinement is the process of refining an initial structure without any

=======

# Oversmoothing and suspended animation

![](HERN_files/animations/caffeine_graph_Gauss-bluring.gif)


## Docking
::: nonincremental
![](HERN_files/images/HADDOCKanimation_of_rigid-body_minimization.gif){.absolute .fragment bottom="110" right="130" width="300"}

![](HERN_files/images/haddock_animation_of_refinement_in_explicit_solvent_water.gif){.absolute top="170" left="30" width="400" height="400"}

![](HERN_files/images/haddock_animation_of_semi-flexible_simulated_annealing.gif){.absolute .fragment top="150" right="80" width="450"}
:::

::: footer
From Bonvin lab: [HADDOCK2.4 Antibody - Antigen tutorial](https://www.bonvinlab.org/education/HADDOCK24/HADDOCK24-antibody-antigen/)
:::

# HERN {.centered-title}

<style>
/* Center slice title */
.centered-title {
  text-align: center;
}
</style>

##

<style>
.center-diagram {
  width: 125%;
  height: 95%;
  display: flex;
  align-items: center;
  justify-content: center;
}
.slide {
  display: flex;
  align-items: center;
  justify-content: center;
}
.node text {
  font-size: 120px;
}
.label text {
  font-size: 30px;
}
</style>

<section class="slide">

<div class="center-diagram">
```{mermaid}
classDiagram
  class EGNNEncoder {
    - update_X: bool
    - features_type: str
    - features: ProteinFeatures
    - node_in: int
    - edge_in: int
    - W_v: nn.Linear
    - W_e: nn.Linear
    - layers: nn.ModuleList
    - W_x: nn.Linear
    - U_x: nn.Linear
    - T_x: nn.Sequential
    + __init__(args, node_hdim=0, features_type='backbone', update_X=True)
    - forward(X, V, S, A)

  }
  class HierEGNNEncoder {
    - update_X: bool
    - backbone_CA_only: bool
    - clash_step: int
    - residue_mpn: EGNNEncoder
    - atom_mpn: EGNNEncoder
    - W_x: nn.Linear
    - U_x: nn.Linear
    - T_x: nn.Sequential
    - W_a: nn.Linear
    - U_a: nn.Linear
    - T_a: nn.Sequential
    - embedding: nn.Embedding
    + __init__(args, update_X=True, backbone_CA_only=True)
    + forward(X, V, S, A)
  }
  class MPNNLayer {
    - num_hidden: int
    - num_in: int
    - dropout: nn.Dropout
    - norm: nn.Identity
    - W: nn.Sequential
    + forward(h_V: torch.Tensor, h_E: torch.Tensor, mask_attend: torch.Tensor) -> torch.Tensor
  }
  class PosEmbedding {
    - num_embeddings: int
    + forward(E_idx: torch.Tensor) -> torch.Tensor
  }
  class AAEmbedding {
    - hydropathy: dict
    - volume: dict
    - charge: dict
    - polarity: dict
    - acceptor: dict
    - donor: dict
    - embedding: torch.Tensor
    + to_rbf(D: torch.Tensor, D_min: float, D_max: float, stride: float) -> torch.Tensor
    + transform(aa_vecs: torch.Tensor) -> torch.Tensor
    + dim() -> int
    + forward(x: torch.Tensor, raw: bool=False) -> torch.Tensor
    + soft_forward(x: torch.Tensor) -> torch.Tensor
  }
  class ABModel {
    - k_neighbors: int
    - hidden_size: int
    - embedding: AAEmbedding
    - features: ProteinFeatures
    - W_i: nn.Linear
    - bce_loss: nn.BCEWithLogitsLoss
    - ce_loss: nn.CrossEntropyLoss
    - mse_loss: nn.MSELoss
    - huber_loss: nn.SmoothL1Loss
    + select_target(tgt_X: torch.Tensor, tgt_h: torch.Tensor, tgt_A: torch.Tensor, tgt_pos: list) -> tuple
}
  class PositionalEncodings {
    -num_embeddings: int
    -period_range: list
    +__init__(num_embeddings, period_range=[2,1000])
  }
  class ProteinFeatures {
    - top_k: int
    - num_rbf: int
    - features_type: str
    - direction: str
    + forward(positions: torch.Tensor, h_V: torch.Tensor, h_E: torch.Tensor, A: torch.Tensor, R: torch.Tensor, pos: list) -> tuple
  }
  class RefineDocker {
    + rstep: int
    + target_mpn: EGNNEncoder
    + hierarchical: bool
    - U_i: nn.Linear
    - struct_mpn: Union[EGNNEncoder, HierEGNNEncoder]
    - W_x0: nn.Sequential
    - U_x0: nn.Sequential
    + struct_loss(bind_X, tgt_X, true_V, true_R, true_D, inter_D, true_C): Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]
    + forward(binder, target, surface): ReturnType
  }
  class CondRefineDecoder {
    -hierarchical: bool
    -residue_atom14: Tensor
    -W_s0: Sequential
    -W_x0: Sequential
    -U_x0: Sequential
    -W_s: Linear
    -U_i: Linear
    -target_mpn: EGNNEncoder
    -struct_mpn: EGNNEncoder or HierEGNNEncoder
    -seq_mpn: EGNNEncoder or HierEGNNEncoder
    +struct_loss(): Function
    +forward(): ReturnType
    +generate(): ReturnType
  }
  class AttRefineDecoder {
    - W_x: nn.Linear
    - W_s: nn.Linear
    - struct_mpn: EGNNEncoder
    - seq_mpn: EGNNEncoder
    - W_x0: nn.Sequential
    - U_i: nn.Linear
    - target_mpn: EGNNEncoder
    - W_att: nn.Sequential
    + attention(Q: Tensor, context: Tensor, cmask: Tensor): Tensor
    + struct_loss(X: Tensor, mask: Tensor, true_D: Tensor, true_V: Tensor, true_R: Tensor, true_C: Tensor): Tuple
    + forward(binder: Tuple, target: Tuple, surface: Tuple): ReturnType
    + generate(target: Tuple, surface: Tuple): ReturnType
  }
  class UncondRefineDecoder {
    - W_x: nn.Linear
    - W_s: nn.Linear
    - struct_mpn: EGNNEncoder
    - seq_mpn: EGNNEncoder
    - W_x0: nn.Sequential
    + struct_loss(X: Tensor, mask: Tensor, true_D: Tensor, true_V: Tensor, true_R: Tensor, true_C: Tensor): Tuple
    + forward(binder: Tuple, target: Tuple, surface: Tuple): ReturnType
    + generate(target: Tuple, surface: Tuple): ReturnType
  }
  class SequenceDecoder {
    - no_target
    - W_s
    - seq_rnn
    - U_i
    - target_mpn
    - W_att
    + attention(Q, context, cmask)
    + forward(binder, target, surface)
    + generate(target, surface)
  }
  
  HierEGNNEncoder *-- EGNNEncoder : init
  ABModel *-- AAEmbedding : W_i = nn.Linear(self.embedding.dim(), args.hidden_size)
  ABModel *-- ProteinFeatures : init features
  RefineDocker *-- EGNNEncoder : init
  CondRefineDecoder *-- EGNNEncoder : init
  AttRefineDecoder *-- EGNNEncoder : init
  ABModel --|> RefineDocker : inherits dihedral features
  ABModel --|> CondRefineDecoder : Inherits
  ABModel --|> AttRefineDecoder : Inherits
  RefineDocker *-- HierEGNNEncoder : init
  CondRefineDecoder *-- HierEGNNEncoder : init
  ABModel --|> UncondRefineDecoder : Inherits
  ABModel --|> SequenceDecoder : Inherits
  PositionalEncodings --* ProteinFeatures : init
  PosEmbedding --* AttRefineDecoder : init
  PosEmbedding --* CondRefineDecoder : init
  PosEmbedding --* UncondRefineDecoder : init
  EGNNEncoder <|-- MPNNLayer : init
  EGNNEncoder <|-- ProteinFeatures : init
```
</div>
</section>

# Quaternions {.centered-title}

  - A 4-number system that utilises complex numbers for geometric transformations/rotations in 3D space 
  - Better than Euler angles
    - Does not suffer from Gimble Lock
      - Gimble Lock is the case when the Euler angles are not unique and the rotation is not well defined

##

![](HERN_files/animations/Quaternions.mp4)